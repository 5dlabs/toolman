# MCP Server Integration Guide: From GitHub URL to Working Tools

**A Comprehensive Guide for AI Agents**

This document provides a systematic approach for integrating new MCP servers into the MCP Bridge Proxy system, based on extensive real-world debugging and integration experience. It covers all edge cases, failure modes, and debugging strategies discovered through testing 25+ different MCP servers.

## üìã Table of Contents

1. [Overview & Architecture](#overview--architecture)
2. [Prerequisites & Environment Setup](#prerequisites--environment-setup)
3. [Phase 1: Initial Analysis](#phase-1-initial-analysis)
4. [Phase 2: Server Type Detection](#phase-2-server-type-detection)
5. [Phase 3: Installation & Testing](#phase-3-installation--testing)
6. [Phase 4: Configuration Integration](#phase-4-configuration-integration)
7. [Phase 5: Tool Discovery & Validation](#phase-5-tool-discovery--validation)
8. [Edge Cases & Troubleshooting](#edge-cases--troubleshooting)
9. [Debugging Strategies](#debugging-strategies)
10. [Quality Assurance Checklist](#quality-assurance-checklist)

---

## Overview & Architecture

### System Architecture
The MCP Bridge Proxy uses a selective tool exposure architecture:

```
AI Agent (Cursor) ‚Üî MCP Bridge Proxy ‚Üî Individual MCP Servers
                         ‚Üì
                servers-config.json
                    ‚Üì
            Only enabled tools visible
```

### Core Principles
1. **Selective Tool Exposure**: Only tools marked `enabled: true` appear in Cursor
2. **Dynamic Tool Management**: Runtime enable/disable without config file edits
3. **Robust Discovery**: Handle broken, non-compliant, and edge-case servers
4. **Zero-Failure Integration**: System continues working even if individual servers fail

---

## Prerequisites & Environment Setup

### Required Tools
Ensure these tools are available before starting:

```bash
# Verify required tools are installed
npx --version          # Node.js package runner
uvx --version          # Python package runner
docker --version       # Container runtime
gh --version           # GitHub CLI
cargo --version        # Rust toolchain
git --version          # Version control
jq --version           # JSON processor
python3 --version      # Python interpreter
```

### Environment Variables
Check that necessary environment variables are available:

```bash
# API Keys (inherit from shell environment)
echo $GITHUB_TOKEN
echo $ANTHROPIC_API_KEY
echo $PERPLEXITY_API_KEY
echo $POSTGRES_CONNECTION_STRING
echo $BRAVE_API_KEY
# ... other provider keys as needed
```

### Working Directory Setup
```bash
# Ensure you're in the MCP Bridge Proxy project directory
cd /path/to/mcp-proxy
ls -la servers-config.json  # Should exist
ls -la src/bin/http_server.rs  # Should exist
```

---

## Phase 1: Initial Analysis

### Step 1.1: GitHub Repository Analysis

Given a GitHub URL like `https://github.com/owner/repo-name`, perform comprehensive analysis:

```bash
# Clone the repository for analysis
REPO_URL="https://github.com/owner/repo-name"
REPO_NAME=$(basename "$REPO_URL" .git)
ANALYSIS_DIR="/tmp/mcp-analysis-$REPO_NAME"

# Clean clone for analysis
rm -rf "$ANALYSIS_DIR"
git clone "$REPO_URL" "$ANALYSIS_DIR"
cd "$ANALYSIS_DIR"
```

### Step 1.2: README Analysis

**Critical**: Always download and analyze the README first:

```bash
# Download README using GitHub CLI (more reliable than git clone)
gh repo view "$REPO_URL" --json readme --jq '.readme.text' > README_CONTENT.md

# Alternative: Use raw GitHub API if gh CLI unavailable
curl -H "Accept: application/vnd.github.v3.raw" \
     "https://api.github.com/repos/owner/repo-name/readme" > README_CONTENT.md
```

**README Analysis Checklist:**
- [ ] Installation command (npm, pip, docker, cargo)
- [ ] Server type indicators (package.json, requirements.txt, Dockerfile, Cargo.toml)
- [ ] Required arguments (file paths, URLs, API keys)
- [ ] Environment variable requirements
- [ ] Usage examples with actual command syntax
- [ ] Dependencies and prerequisites
- [ ] Platform compatibility (Windows/Mac/Linux)

### Step 1.3: Repository Structure Analysis

```bash
# Analyze file structure for server type detection
find . -name "package.json" -o -name "requirements.txt" -o -name "Dockerfile" -o -name "Cargo.toml" -o -name "setup.py" -o -name "pyproject.toml"

# Check for build requirements
ls -la setup.sh install.sh build.sh package.json
ls -la src/ dist/ build/ target/
```

---

## Phase 2: Server Type Detection

### Step 2.1: Automatic Server Type Detection

Use this decision tree based on repository analysis:

```bash
# Function to detect server type
detect_server_type() {
    local repo_dir="$1"

    # NPM Package (highest priority for official MCP servers)
    if [[ -f "$repo_dir/package.json" ]]; then
        local package_name=$(jq -r '.name // empty' "$repo_dir/package.json")
        if [[ -n "$package_name" ]]; then
            echo "npm:$package_name"
            return
        fi
    fi

    # Python Package
    if [[ -f "$repo_dir/pyproject.toml" ]] || [[ -f "$repo_dir/requirements.txt" ]] || [[ -f "$repo_dir/setup.py" ]]; then
        # Check for package name in pyproject.toml
        if [[ -f "$repo_dir/pyproject.toml" ]]; then
            local package_name=$(grep -E "^name\s*=" "$repo_dir/pyproject.toml" | cut -d'"' -f2)
            if [[ -n "$package_name" ]]; then
                echo "python:$package_name"
                return
            fi
        fi
        echo "python:source"
        return
    fi

    # Docker Container
    if [[ -f "$repo_dir/Dockerfile" ]]; then
        echo "docker:source"
        return
    fi

    # Rust Binary
    if [[ -f "$repo_dir/Cargo.toml" ]]; then
        local bin_name=$(grep -E "^\[\[bin\]\]" -A 2 "$repo_dir/Cargo.toml" | grep "name" | cut -d'"' -f2)
        if [[ -n "$bin_name" ]]; then
            echo "rust:$bin_name"
        else
            echo "rust:source"
        fi
        return
    fi

    # Source build required
    echo "source:unknown"
}
```

### Step 2.2: Package Name Extraction

**Critical**: Extract the correct package name from repository metadata:

```bash
# For NPM packages
extract_npm_package_name() {
    local repo_dir="$1"

    # Check package.json first
    if [[ -f "$repo_dir/package.json" ]]; then
        jq -r '.name // empty' "$repo_dir/package.json"
        return
    fi

    # Check README for npx commands
    grep -E "npx\s+[^-]" "$repo_dir/README.md" | head -1 | awk '{print $2}'
}

# For Python packages
extract_python_package_name() {
    local repo_dir="$1"

    # Check pyproject.toml
    if [[ -f "$repo_dir/pyproject.toml" ]]; then
        grep -E "^name\s*=" "$repo_dir/pyproject.toml" | cut -d'"' -f2
        return
    fi

    # Check setup.py
    if [[ -f "$repo_dir/setup.py" ]]; then
        grep -E "name\s*=" "$repo_dir/setup.py" | cut -d'"' -f2 | head -1
        return
    fi

    # Check README for uvx commands
    grep -E "uvx\s+[^-]" "$repo_dir/README.md" | head -1 | awk '{print $2}'
}
```

---

## Phase 3: Installation & Testing

### Step 3.1: Server Installation Strategy

**Test installation in isolation first** before adding to configuration:

```bash
# NPM Package Testing
test_npm_package() {
    local package_name="$1"
    local test_args="$2"

    echo "Testing NPM package: $package_name"

    # Test package availability
    if ! npx -y "$package_name" --help 2>/dev/null; then
        echo "‚ùå Package not available or broken: $package_name"
        return 1
    fi

    # Test with minimal arguments
    timeout 10 npx -y "$package_name" $test_args 2>&1 | head -5
}

# Python Package Testing
test_python_package() {
    local package_name="$1"
    local test_args="$2"

    echo "Testing Python package: $package_name"

    # Test package availability
    if ! uvx "$package_name" --help 2>/dev/null; then
        echo "‚ùå Package not available or broken: $package_name"
        return 1
    fi

    # Test with minimal arguments
    timeout 10 uvx "$package_name" $test_args 2>&1 | head -5
}
```

### Step 3.2: Source Build Testing

For servers requiring source builds:

```bash
# Source Build Testing
test_source_build() {
    local repo_url="$1"
    local repo_name=$(basename "$repo_url" .git)
    local build_dir="/tmp/mcp-build-$repo_name"

    echo "Testing source build: $repo_url"

    # Clean build environment
    rm -rf "$build_dir"
    git clone "$repo_url" "$build_dir"
    cd "$build_dir"

    # Try different build methods
    if [[ -f "setup.sh" ]]; then
        echo "Found setup.sh, running..."
        bash setup.sh
    elif [[ -f "package.json" ]]; then
        echo "Found package.json, running npm install..."
        npm install
        if [[ -f "build.sh" ]]; then
            bash build.sh
        else
            npm run build 2>/dev/null || true
        fi
    elif [[ -f "Cargo.toml" ]]; then
        echo "Found Cargo.toml, running cargo build..."
        cargo build --release
    fi

    # Test the built server
    if [[ -f "dist/index.js" ]]; then
        timeout 10 node dist/index.js --help 2>&1 | head -5
    elif [[ -f "target/release/"* ]]; then
        local binary=$(find target/release -type f -executable | head -1)
        timeout 10 "$binary" --help 2>&1 | head -5
    fi
}
```

### Step 3.3: MCP Protocol Compliance Testing

**Critical**: Test MCP protocol compliance before integration:

```bash
# MCP Protocol Test
test_mcp_compliance() {
    local command="$1"
    local args="$2"

    echo "Testing MCP protocol compliance..."

    # Create test requests
    local init_request='{"id":1,"jsonrpc":"2.0","method":"initialize","params":{"capabilities":{},"clientInfo":{"name":"test","version":"1.0.0"},"protocolVersion":"2024-11-05"}}'
    local initialized_notification='{"jsonrpc":"2.0","method":"notifications/initialized"}'
    local tools_request='{"id":2,"jsonrpc":"2.0","method":"tools/list","params":{}}'

    # Test server response
    local response=$(timeout 30 bash -c "
        echo -e '$init_request\n$initialized_notification\n$tools_request' | $command $args 2>/dev/null
    ")

    # Analyze response
    if echo "$response" | grep -q '"jsonrpc":"2.0"'; then
        echo "‚úÖ Server responds with valid JSON-RPC"

        # Count tools
        local tool_count=$(echo "$response" | jq -r '.result.tools[]?.name' 2>/dev/null | wc -l)
        echo "‚úÖ Found $tool_count tools"

        return 0
    else
        echo "‚ùå Server does not respond with valid JSON-RPC"
        echo "Raw response: $response"
        return 1
    fi
}
```

---

## Phase 4: Configuration Integration

### Step 4.1: Configuration Entry Generation

Based on successful testing, generate configuration entry:

```bash
# Generate configuration entry
generate_config_entry() {
    local server_name="$1"
    local server_type="$2"  # npm, python, docker, rust, source
    local package_or_command="$3"
    local args="$4"
    local description="$5"
    local repo_url="$6"

    cat << EOF
    "$server_name": {
      "name": "$description",
      "description": "$description",
      "repo": "$repo_url",
      "type": "$server_type",
      "command": "$(get_command_for_type "$server_type")",
      "args": $(echo "$args" | jq -R 'split(" ") | map(select(length > 0))'),
      "tools": {}
    }
EOF
}

get_command_for_type() {
    case "$1" in
        npm) echo "npx" ;;
        python) echo "uvx" ;;
        docker) echo "docker" ;;
        rust|source) echo "$(which "$package_or_command")" ;;
        *) echo "unknown" ;;
    esac
}
```

### Step 4.2: Environment Variable Handling

**Critical Rule**: Never put environment variables in servers-config.json:

```bash
# ‚ùå DON'T: Add env section to config
{
  "env": {
    "API_KEY": "value"
  }
}

# ‚úÖ DO: Let servers inherit from shell environment
# Environment variables should be set in:
# - .env file (for CLI usage)
# - .cursor/mcp.json env section (for Cursor integration)
```

### Step 4.3: Argument Processing

Handle different argument patterns:

```bash
# Process arguments based on server requirements
process_server_args() {
    local server_type="$1"
    local repo_analysis="$2"

    case "$server_type" in
        npm)
            # NPM packages: usually just package name
            echo '"-y", "package-name"'
            ;;
        python)
            # Python packages: package name + optional args
            echo '"package-name"'
            ;;
        docker)
            # Docker: run command + image + args
            echo '"run", "-i", "--rm", "image-name"'
            ;;
        source)
            # Source builds: absolute path to binary
            echo '"/absolute/path/to/binary"'
            ;;
    esac
}
```

---

## Phase 5: Tool Discovery & Validation

### Step 5.1: Tool Discovery Process

Use the bridge proxy's discovery system:

```bash
# Run tool discovery for new server
./target/release/http_server --export-tools discovered-tools-new.json --project-dir $(pwd)

# Check if new server was discovered
jq '.servers[] | select(.name == "new-server-name") | .tools_count' discovered-tools-new.json
```

### Step 5.2: Configuration Synchronization

**Critical**: Always sync discovered tools with configuration:

```bash
# Update configuration with discovered tools
python3 update-config.py discovered-tools-new.json servers-config.json

# Verify synchronization
python3 compare-tools.py discovered-tools-new.json servers-config.json
```

### Step 5.3: Tool Validation

Validate that tools have proper schemas:

```bash
# Check tool schemas
validate_tool_schemas() {
    local discovered_file="$1"

    jq -r '.servers[] | .name as $server | .tools[] |
           "\($server): \(.name) - \(if .inputSchema then "‚úÖ" else "‚ùå" end) Schema"' \
           "$discovered_file"
}
```

---

## Edge Cases & Troubleshooting

### Edge Case 1: Multi-line Response Servers

**Problem**: Server prints status messages before JSON responses

**Example**:
```
MCP Documentation Management Service started.    ‚Üê Status message
Using docs directory: /path/to/docs              ‚Üê Status message
{"jsonrpc":"2.0","id":1,"result":{...}}          ‚Üê Actual JSON
```

**Solution**: The bridge proxy handles this with multi-line reading:
```rust
// Keep reading lines until valid JSON is found
loop {
    line.clear();
    match reader.read_line(&mut line).await {
        Ok(bytes_read) => {
            if let Ok(_) = serde_json::from_str::<Value>(&line) {
                break; // Found valid JSON
            }
            // Continue reading next line
        }
    }
}
```

**Detection**: If discovery shows 0 tools but manual testing works

### Edge Case 2: Source Build Requirements

**Problem**: Server not available as package, requires compilation

**Examples**:
- `markdownify-mcp`: Requires cloning and building
- `vibe-check-mcp-server`: Requires npm install + build

**Solution Process**:
```bash
# 1. Clone repository
git clone https://github.com/user/server-repo /tmp/build-server
cd /tmp/build-server

# 2. Run build process
if [[ -f "setup.sh" ]]; then
    bash setup.sh
elif [[ -f "package.json" ]]; then
    npm install
    npm run build
fi

# 3. Test built server
node dist/index.js --help

# 4. Update config with absolute path
"command": "node",
"args": ["/absolute/path/to/dist/index.js"]
```

### Edge Case 3: Package Name Mismatches

**Problem**: Published package name differs from repository name

**Examples**:
- Repository: `context7` ‚Üí Package: `@upstash/context7-mcp`
- Repository: `postgres-mcp` ‚Üí Package: `postgres-mcp` (correct)

**Detection Strategy**:
```bash
# Check package.json for actual package name
jq -r '.name' package.json

# Check README for installation commands
grep -E "(npx|uvx)\s+" README.md

# Test package availability
npx -y package-name --help
```

### Edge Case 4: Directory Argument Requirements

**Problem**: Server requires directory path arguments

**Examples**:
- `docs-service`: Requires docs directory path
- `filesystem`: May require working directory

**Solution**:
```bash
# Use absolute paths in configuration
"args": ["-y", "mcp-docs-service", "/absolute/path/to/docs"]

# Not relative paths
"args": ["-y", "mcp-docs-service", "./docs"]  # ‚ùå May fail
```

### Edge Case 5: Initialization Timing

**Problem**: Server needs time to initialize before responding

**Examples**:
- Document scanning servers (docs-service)
- Database connection servers
- Large file processing servers

**Solution**: Bridge proxy includes initialization delay:
```rust
// Give server time to initialize
tokio::time::sleep(tokio::time::Duration::from_secs(3)).await;
```

### Edge Case 6: Non-MCP Compliant Servers

**Problem**: Server doesn't follow MCP protocol correctly

**Detection**:
- Returns plain text instead of JSON
- Missing required fields in responses
- Incorrect JSON-RPC structure

**Solution**: Bridge proxy tolerates some non-compliance:
```rust
// Continue despite non-JSON init response
if let Err(e) = serde_json::from_str::<Value>(&line) {
    println!("‚ö†Ô∏è Non-JSON response, continuing anyway...");
}
```

---

## Debugging Strategies

### Strategy 1: Layered Testing Approach

**Always test in this order**:

1. **Package availability**: `npx -y package-name --help`
2. **Basic execution**: `npx -y package-name args`
3. **MCP protocol**: Manual JSON-RPC testing
4. **Bridge proxy discovery**: `--export-tools`
5. **Configuration sync**: `update-config.py`
6. **End-to-end**: Full system test

### Strategy 2: Individual Server Testing

**Test servers in isolation**:
```bash
# Test individual server manually
echo '{"id":1,"jsonrpc":"2.0","method":"initialize","params":{"capabilities":{},"clientInfo":{"name":"test","version":"1.0.0"},"protocolVersion":"2024-11-05"}}
{"jsonrpc":"2.0","method":"notifications/initialized"}
{"id":2,"jsonrpc":"2.0","method":"tools/list","params":{}}' | npx -y server-name args
```

### Strategy 3: Bridge Proxy Debugging

**Use verbose discovery output**:
```bash
# Run discovery with full output
./target/release/http_server --export-tools /dev/null --project-dir $(pwd) 2>&1 | grep -A 20 "server-name"
```

**Look for these patterns**:
- `‚ùå Failed to spawn process` ‚Üí Installation issue
- `‚ùå No init response received` ‚Üí Server crash or wrong args
- `‚ùå Invalid JSON` ‚Üí Multi-line response issue
- `‚úÖ Found X tools` ‚Üí Success

### Strategy 4: Configuration Validation

**Always validate configuration changes**:
```bash
# Check JSON syntax
jq '.' servers-config.json > /dev/null

# Verify server entries
jq '.servers | keys[]' servers-config.json

# Check for missing fields
jq '.servers[] | select(.command == null or .args == null)' servers-config.json
```

### Strategy 5: Environment Debugging

**Check environment inheritance**:
```bash
# Print all environment variables
printenv | grep -E "(API_KEY|TOKEN|CONNECTION)" | sort

# Test environment variable access
echo $GITHUB_TOKEN | wc -c  # Should be > 1 if set
```

---

## Quality Assurance Checklist

### Pre-Integration Checklist

- [ ] **Repository Analysis Complete**
  - [ ] README downloaded and analyzed
  - [ ] Server type detected correctly
  - [ ] Package name extracted accurately
  - [ ] Required arguments identified
  - [ ] Environment variables documented

- [ ] **Installation Testing Complete**
  - [ ] Package/binary installs successfully
  - [ ] Server responds to `--help`
  - [ ] Basic execution works with test arguments
  - [ ] No obvious errors or crashes

- [ ] **MCP Protocol Testing Complete**
  - [ ] Server responds to initialize request
  - [ ] Server handles initialized notification
  - [ ] Server returns tools list
  - [ ] Response contains valid JSON-RPC
  - [ ] Tools have proper names and descriptions

### Post-Integration Checklist

- [ ] **Configuration Integration Complete**
  - [ ] Server added to servers-config.json
  - [ ] Configuration syntax is valid JSON
  - [ ] Command and args are correct
  - [ ] No environment variables in config
  - [ ] Tools section initialized (empty is OK)

- [ ] **Discovery Validation Complete**
  - [ ] Bridge proxy discovers server successfully
  - [ ] Tool count matches expected number
  - [ ] All tools have valid schemas
  - [ ] No broken pipe or timeout errors
  - [ ] Discovery completes within reasonable time

- [ ] **Configuration Synchronization Complete**
  - [ ] `update-config.py` runs successfully
  - [ ] All discovered tools added to configuration
  - [ ] `compare-tools.py` shows perfect match
  - [ ] No missing or extra tools reported

### Final Validation Checklist

- [ ] **End-to-End Testing Complete**
  - [ ] Full discovery export runs successfully
  - [ ] Server appears in discovery results
  - [ ] Tool count is accurate and > 0
  - [ ] Configuration comparison passes
  - [ ] No errors in bridge proxy logs

- [ ] **Documentation Updated**
  - [ ] Server added to inventory documentation
  - [ ] Any special requirements noted
  - [ ] Edge cases documented if applicable
  - [ ] Integration notes added for future reference

---

## Integration Workflow Template

Use this complete workflow for any new GitHub repository:

```bash
#!/bin/bash
# MCP Server Integration Workflow

set -e  # Exit on any error

REPO_URL="$1"
if [[ -z "$REPO_URL" ]]; then
    echo "Usage: $0 <github-repo-url>"
    exit 1
fi

REPO_NAME=$(basename "$REPO_URL" .git)
echo "üöÄ Starting integration for: $REPO_NAME"

# Phase 1: Analysis
echo "üìã Phase 1: Repository Analysis"
ANALYSIS_DIR="/tmp/mcp-analysis-$REPO_NAME"
rm -rf "$ANALYSIS_DIR"
git clone "$REPO_URL" "$ANALYSIS_DIR"
cd "$ANALYSIS_DIR"

# Download README
gh repo view "$REPO_URL" --json readme --jq '.readme.text' > README_CONTENT.md || {
    echo "‚ö†Ô∏è Could not download README via GitHub CLI, using git clone version"
    cp README.md README_CONTENT.md 2>/dev/null || echo "‚ùå No README found"
}

# Detect server type
SERVER_TYPE=$(detect_server_type "$ANALYSIS_DIR")
echo "üîç Detected server type: $SERVER_TYPE"

# Phase 2: Installation Testing
echo "üß™ Phase 2: Installation Testing"
case "$SERVER_TYPE" in
    npm:*)
        PACKAGE_NAME="${SERVER_TYPE#npm:}"
        test_npm_package "$PACKAGE_NAME" ""
        ;;
    python:*)
        PACKAGE_NAME="${SERVER_TYPE#python:}"
        test_python_package "$PACKAGE_NAME" ""
        ;;
    source:*)
        test_source_build "$REPO_URL"
        ;;
esac

# Phase 3: MCP Protocol Testing
echo "üîå Phase 3: MCP Protocol Testing"
# ... protocol testing logic ...

# Phase 4: Configuration Integration
echo "‚öôÔ∏è Phase 4: Configuration Integration"
# ... configuration generation logic ...

# Phase 5: Discovery & Validation
echo "üîç Phase 5: Discovery & Validation"
cd /path/to/mcp-proxy
./target/release/http_server --export-tools discovered-tools-new.json --project-dir $(pwd)
python3 update-config.py discovered-tools-new.json servers-config.json
python3 compare-tools.py discovered-tools-new.json servers-config.json

echo "‚úÖ Integration complete for: $REPO_NAME"
```

---

## Conclusion

This guide represents the distilled knowledge from integrating 25+ diverse MCP servers with various edge cases, failure modes, and architectural patterns. The key to successful integration is:

1. **Systematic Analysis**: Always start with thorough repository and README analysis
2. **Isolated Testing**: Test each component in isolation before integration
3. **Robust Error Handling**: Expect and handle non-compliant servers gracefully
4. **Configuration Synchronization**: Maintain perfect alignment between discovery and configuration
5. **Comprehensive Validation**: Test every step and validate all assumptions

The MCP Bridge Proxy's architecture is designed to handle these edge cases automatically, but understanding the underlying patterns helps with debugging and extending the system for new server types.

**Remember**: Every MCP server is different. This guide provides the framework, but always be prepared to adapt based on the specific requirements and quirks of each individual server.